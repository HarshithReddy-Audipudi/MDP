{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea3a1698-16d7-49f6-b41b-adef69615620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a normalized database (3NF)\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Define database connection\n",
    "db_conn = sqlite3.connect('health.db')\n",
    "\n",
    "# Create Patients table\n",
    "db_conn.execute('''CREATE TABLE IF NOT EXISTS Patients (\n",
    "    patient_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT,\n",
    "    age INTEGER,\n",
    "    sex TEXT\n",
    ")''')\n",
    "\n",
    "# Create TestResults table\n",
    "db_conn.execute('''CREATE TABLE IF NOT EXISTS TestResults (\n",
    "    result_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    patient_id INTEGER,\n",
    "    disease TEXT,\n",
    "    test_features TEXT,\n",
    "    FOREIGN KEY (patient_id) REFERENCES Patients(patient_id)\n",
    ")''')\n",
    "\n",
    "# Create DiseaseStatus table\n",
    "db_conn.execute('''CREATE TABLE IF NOT EXISTS DiseaseStatus (\n",
    "    status_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    patient_id INTEGER,\n",
    "    disease TEXT,\n",
    "    target_status INTEGER,\n",
    "    FOREIGN KEY (patient_id) REFERENCES Patients(patient_id)\n",
    ")''')\n",
    "\n",
    "db_conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cbc1abf-49ab-4ae5-b02b-ff5f15e40f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "diabetes_df = pd.read_csv('diabetes.csv')\n",
    "heart_df = pd.read_csv('heart.csv')\n",
    "parkinsons_df = pd.read_csv('parkinsons.csv')\n",
    "\n",
    "# Insert data into tables (example for diabetes dataset)\n",
    "for _, row in diabetes_df.iterrows():\n",
    "    db_conn.execute(\"INSERT INTO Patients (name, age, sex) VALUES (?, ?, ?)\", \n",
    "                   (\"Unknown\", row['Age'], \"Unknown\"))\n",
    "    patient_id = db_conn.execute(\"SELECT last_insert_rowid()\").fetchone()[0]\n",
    "    db_conn.execute(\"INSERT INTO TestResults (patient_id, disease, test_features) VALUES (?, ?, ?)\", \n",
    "                   (patient_id, \"Diabetes\", str(row.to_dict())))\n",
    "    db_conn.execute(\"INSERT INTO DiseaseStatus (patient_id, disease, target_status) VALUES (?, ?, ?)\", \n",
    "                   (patient_id, \"Diabetes\", row['Outcome']))\n",
    "\n",
    "db_conn.commit()\n",
    "\n",
    "db_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff48fd5f-80c3-4dcb-8da9-eaf4ba625996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: SQL join statement to fetch data into Pandas DataFrame\n",
    "def fetch_data():\n",
    "    db_conn = sqlite3.connect('health.db')\n",
    "    query = '''\n",
    "    SELECT p.patient_id, p.name, p.age, p.sex, t.disease, t.test_features, d.target_status\n",
    "    FROM Patients p\n",
    "    JOIN TestResults t ON p.patient_id = t.patient_id\n",
    "    JOIN DiseaseStatus d ON p.patient_id = d.patient_id\n",
    "    '''\n",
    "    df = pd.read_sql_query(query, db_conn)\n",
    "    db_conn.close()\n",
    "    return df\n",
    "\n",
    "data = fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5cf6fe3-535c-4501-abc3-529efc870321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Explore data for stratification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(columns=['target_status'])\n",
    "y = data['target_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0df5056-1dde-4c47-8b45-17a424272457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f0f538e-c879-4f8c-b114-84f358938315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712b913004224d3db1d6d8d5b0ca879e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93382621289943dea46d196ebe96309a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488cb49915e24ee2978c1e43ac43aff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddc730b564544ca8ddf15c0a2e22760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix heatmap saved as 'correlation_matrix.png'\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Data exploration with yprofile and correlation matrix\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate yprofile report\n",
    "profile = ProfileReport(X_train, title=\"Data Profile Report\")\n",
    "profile.to_file(\"eda_report.html\")\n",
    "\n",
    "# Correlation matrix\n",
    "# Select only numeric columns\n",
    "numeric_columns = X_train.select_dtypes(include=['float64', 'int64'])\n",
    "corr_matrix = numeric_columns.corr()\n",
    "\n",
    "# Plot correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix for Numeric Features\")\n",
    "plt.savefig(\"correlation_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Correlation matrix heatmap saved as 'correlation_matrix.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12b52688-40ca-48fd-8ed6-b02c07fc6050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X before dropping NaNs: (768, 6)\n",
      "Shape of y before dropping NaNs: (768,)\n",
      "Shape of X_train: (614, 6), y_train: (614,)\n",
      "Shape of X_test: (154, 6), y_test: (154,)\n",
      "F1-score: Mean=0.2537, Std=0.0961\n"
     ]
    }
   ],
   "source": [
    "# Ensure no missing values in X or y\n",
    "print(f\"Shape of X before dropping NaNs: {X.shape}\")\n",
    "print(f\"Shape of y before dropping NaNs: {y.shape}\")\n",
    "\n",
    "# Drop rows with NaN values in the target column to align X and y\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# Stratified train-test split with alignment\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Updated Preprocessing Pipeline and Logistic Regression\n",
    "numerical_features = ['age']  # Add relevant numerical features\n",
    "categorical_features = ['sex']  # Add relevant categorical features\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(), categorical_features)\n",
    "])\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "cv_results = cross_val_score(pipeline, X_train, y_train, cv=10, scoring='f1')\n",
    "print(f\"F1-score: Mean={cv_results.mean():.4f}, Std={cv_results.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0415e5b4-97ce-41c2-a834-bd647df0dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "186129e8-66f0-44d6-ae71-2f4f354d1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"HarshithReddy-Audipudi\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"Qwerty@123\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07dac8a7-3814-4580-b739-486050fcb50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/17 23:19:50 INFO mlflow.tracking.fluent: Experiment with name 'Multiple Disease Prediction' does not exist. Creating a new experiment.\n",
      "\u001b[31m2024/12/17 23:19:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run illustrious-lamb-347 at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0/runs/9e731eded9dd468696ee31649123aa1c\n",
      "ðŸ§ª View experiment at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Log results in MLFlow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow\")\n",
    "mlflow.set_experiment(\"Multiple Disease Prediction\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    mlflow.log_param(\"model_type\", \"Logistic Regression\")\n",
    "    mlflow.log_metric(\"cv_f1_mean\", cv_results.mean())\n",
    "    mlflow.log_metric(\"cv_f1_std\", cv_results.std())\n",
    "    mlflow.sklearn.log_model(pipeline, \"logistic_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "646e4157-048f-4a81-befd-9f0c4a774e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/17 23:20:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run fortunate-gnat-846 at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0/runs/c6303df1537348c29e5b02e1d5c4f537\n",
      "ðŸ§ª View experiment at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/17 23:20:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run thoughtful-jay-903 at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0/runs/3fcbb03df0b24ab0bcaccc2e54f6bc4b\n",
      "ðŸ§ª View experiment at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/17 23:20:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run kindly-hound-390 at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0/runs/04a48f0f12fa44608066155ba407e8fc\n",
      "ðŸ§ª View experiment at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/17 23:21:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run inquisitive-gull-841 at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0/runs/405002a2c2c74ef7839bb1f41251e723\n",
      "ðŸ§ª View experiment at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Experiment #2 with multiple classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Ridge Classifier\": RidgeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost Classifier\": XGBClassifier()\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "\n",
    "    cv_results = cross_val_score(pipeline, X_train, y_train, cv=10, scoring='f1')\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "        mlflow.log_metric(\"cv_f1_mean\", cv_results.mean())\n",
    "        mlflow.log_metric(\"cv_f1_std\", cv_results.std())\n",
    "        mlflow.sklearn.log_model(pipeline, f\"{name}_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d54a9cf3-bc20-4c6b-b5d8-94e5ea694281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scree plot saved as 'scree_plot.png'\n",
      "ðŸƒ View run classy-asp-494 at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0/runs/f99bfb86b37f4cb0821e8071efc38fb3\n",
      "ðŸ§ª View experiment at: https://dagshub.com/HarshithReddy-Audipudi/MDP.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Feature Engineering and PCA (Experiment #3 and #5)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train.select_dtypes(include=['float64', 'int64']))\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Scree Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.savefig(\"scree_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Scree plot saved as 'scree_plot.png'\")\n",
    "\n",
    "# Log PCA results\n",
    "with mlflow.start_run():\n",
    "    for i, ev in enumerate(explained_variance):\n",
    "        mlflow.log_metric(f\"PCA_Component_{i+1}\", ev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b35ddb6-f91d-4173-b07f-20f80b26f78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_model.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 9: Save the final model\n",
    "import joblib\n",
    "\n",
    "final_model = pipeline.fit(X_train, y_train)\n",
    "joblib.dump(final_model, \"final_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f2227ec-5f08-4741-a8f7-8b4ce5e91ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: FastAPI application\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "\n",
    "app = FastAPI()\n",
    "model = joblib.load(\"final_model.joblib\")\n",
    "\n",
    "class ModelInput(BaseModel):\n",
    "    features: list\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(input: ModelInput):\n",
    "    prediction = model.predict([input.features])\n",
    "    return {\"prediction\": prediction.tolist()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63911708-4746-4bc3-ab7e-2be4db86abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Dockerize the application\n",
    "# Create Dockerfile as follows:\n",
    "# FROM python:3.9-slim\n",
    "# WORKDIR /app\n",
    "# COPY . .\n",
    "# RUN pip install fastapi uvicorn scikit-learn joblib pydantic\n",
    "# CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\n",
    "# Step 12: Deploy the containerized API\n",
    "# Push the Docker container to Docker Hub and deploy it on a cloud platform like AWS/GCP/Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e5a30d3-04a8-4e2b-96ee-cb102e4fb806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Streamlit app for real-time interaction\n",
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "st.title(\"Multiple Disease Prediction\")\n",
    "\n",
    "features = st.text_input(\"Enter test features (comma-separated):\")\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    try:\n",
    "        # Convert input features to a list of floats\n",
    "        feature_list = [float(x.strip()) for x in features.split(\",\")]\n",
    "        \n",
    "        # Send request to FastAPI endpoint\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8000/predict\",\n",
    "            json={\"features\": feature_list}\n",
    "        )\n",
    "        \n",
    "        # Display prediction result\n",
    "        if response.status_code == 200:\n",
    "            prediction = response.json()['prediction']\n",
    "            st.success(f\"Prediction: {prediction}\")\n",
    "        else:\n",
    "            st.error(f\"Error: {response.text}\")\n",
    "    except ValueError:\n",
    "        st.error(\"Please enter valid numeric inputs separated by commas.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "583ae570-b44f-4550-a92f-aa336f2826c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Steps: Create presentation video and JupyterBook\n",
    "# Record a video walkthrough of the project and compile the code, results, and video into a JupyterBook website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254cbeb-a4a8-4cd6-9b35-1bd3a2511477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
